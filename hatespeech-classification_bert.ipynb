{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data, drop empty Txt and Settings\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        comment_text  toxic\n0  u created request scholarlyarticles awaiting c...      1\n1  looking lgbt profession category point get beh...      1\n2  new york city regulation regarding removal, , ...      0\n3  thank speedy rollback would believe first time...      1\n4  want talk stuff perfectly willing refer commen...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>u created request scholarlyarticles awaiting c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>looking lgbt profession category point get beh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>new york city regulation regarding removal, , ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>thank speedy rollback would believe first time...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>want talk stuff perfectly willing refer commen...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SET_datapath = \"data_pp_cut.json\"\n",
    "\n",
    "SET_SVM_active = False\n",
    "\n",
    "df = pd.read_json(SET_datapath, orient=\"columns\")\n",
    "\n",
    "df.drop(df[df.comment_text.str.len() == 0].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Embedding\n",
    "\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2241737607.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[14], line 7\u001B[1;36m\u001B[0m\n\u001B[1;33m    for i in rang\u001B[0m\n\u001B[1;37m                 ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input comments\n",
    "for i in rang\n",
    "encoded_comments = tokenizer(df['comment_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Pass tokenized comments through BERT model to get embeddings\n",
    "bert_embeddings = model(encoded_comments)['pooler_output'].numpy()\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input comments\n",
    "encoded_comments = tokenizer(df['comment_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Pass tokenized comments through BERT model to get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_comments)\n",
    "    bert_embeddings = outputs.pooler_output.numpy()\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "batch_size = 2000\n",
    "bert_embeddings = []\n",
    "\n",
    "# Tokenize and process comments in batches\n",
    "for i in range(0, len(df), batch_size):\n",
    "    comments_batch = df['comment_text'].iloc[i:i+batch_size].tolist()\n",
    "    encoded_comments = tokenizer(comments_batch, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "    batch_embeddings = model(encoded_comments)['pooler_output'].numpy()\n",
    "    bert_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate embeddings from all batches\n",
    "bert_embeddings = np.concatenate(bert_embeddings, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "batch_size = 2000\n",
    "bert_embeddings = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and process comments in batches\n",
    "for i in range(0, len(df), batch_size):\n",
    "    comments_batch = df['comment_text'].iloc[i:i+batch_size].tolist()\n",
    "    encoded_comments = tokenizer(comments_batch, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "bert_embeddings = model(encoded_comments)['pooler_output'].numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BertDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, toxic_labels):\n",
    "        self.sentences = sentences\n",
    "        #target is a matrix with shape [#1 x #6(toxic, obscene, etc)]\n",
    "        self.targets = toxic_labels.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        bert_senten = tokenizer.encode_plus(sentence,\n",
    "                                            add_special_tokens = True, # [CLS],[SEP]\n",
    "                                            max_length = max_len,\n",
    "                                            pad_to_max_length = True,\n",
    "                                            truncation = True,\n",
    "                                            return_attention_mask = True\n",
    "                                             )\n",
    "        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n",
    "        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n",
    "        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids' : ids,\n",
    "            'mask' : mask,\n",
    "            'toxic_label':toxic_label\n",
    "        }\n",
    "\n",
    "train_dataset = BertDataSet(df['comment_text'], df[['toxic']])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = train_batch, pin_memory = True, num_workers = 4, shuffle = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import transformers\n",
    "%%time\n",
    "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 6)\n",
    "model.to(device)\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "for a in train_dataloader:\n",
    "    ids = a['ids'].to(device)\n",
    "    mask = a['mask'].to(device)\n",
    "    output = model(ids, mask)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Data for Models\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, df['toxic'].values, test_size=0.2, random_state=99)\n",
    "\n",
    "# Scale input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T09:06:44.894984Z",
     "iopub.execute_input": "2023-03-08T09:06:44.895705Z",
     "iopub.status.idle": "2023-03-08T09:06:44.902237Z",
     "shell.execute_reply.started": "2023-03-08T09:06:44.895669Z",
     "shell.execute_reply": "2023-03-08T09:06:44.900681Z"
    },
    "trusted": true,
    "id": "BXQp4lTzC1OK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Models and get Scores\n",
    "\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Try... Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# load the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# convert text to numerical data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)\n",
    "X_train = tokenizer.texts_to_matrix(newsgroups_train.data, mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(newsgroups_test.data, mode='tfidf')\n",
    "\n",
    "# convert labels to categorical data\n",
    "num_classes = np.max(newsgroups_train.target) + 1\n",
    "y_train = to_categorical(newsgroups_train.target, num_classes)\n",
    "y_test = to_categorical(newsgroups_test.target, num_classes)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(2000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "#model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def model_train(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_tr = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test,y_pred)\n",
    "    recall = report.split()[-2]\n",
    "    print(\"--------------------Training Performance---------------------\")\n",
    "    print(accuracy_score(y_train,y_pred_tr))\n",
    "    print(classification_report(y_train,y_pred_tr))\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(\"--------------------Testing Performance----------------------\")\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),cmap='viridis',annot=True,fmt='.4g')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.show()\n",
    "\n",
    "    # Save recall value in a CSV file\n",
    "    model_name = \"bert_\" + model.__class__.__name__\n",
    "    new_recall = recall\n",
    "\n",
    "    # read existing data from csv file\n",
    "    existing_data = []\n",
    "    with open(\"performance.csv\", mode=\"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            existing_data.append(row)\n",
    "\n",
    "    # update recall value if modelname already exists\n",
    "    for row in existing_data:\n",
    "        if row[\"Model\"] == model_name:\n",
    "            row[\"Recall\"] = new_recall\n",
    "\n",
    "    # write data back to csv file\n",
    "    with open(\"performance.csv\", mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"Model\", \"Recall\"])\n",
    "        writer.writeheader()\n",
    "        for row in existing_data:\n",
    "            writer.writerow(row)\n",
    "        # write new row if model doesn't exist\n",
    "        if model_name not in [row[\"Model\"] for row in existing_data]:\n",
    "            writer.writerow({\"Model\": model_name, \"Recall\": new_recall})"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T09:06:48.069063Z",
     "iopub.execute_input": "2023-03-08T09:06:48.069767Z",
     "iopub.status.idle": "2023-03-08T09:06:48.077451Z",
     "shell.execute_reply.started": "2023-03-08T09:06:48.069730Z",
     "shell.execute_reply": "2023-03-08T09:06:48.076100Z"
    },
    "trusted": true,
    "id": "eB0eFrl8C1OK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Naive Bayes:"
   ],
   "metadata": {
    "id": "tfuKOJNrC1OL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "modelNB = MultinomialNB()\n",
    "model_train(modelNB, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:07:23.062257Z",
     "iopub.execute_input": "2023-03-08T08:07:23.062625Z",
     "iopub.status.idle": "2023-03-08T08:07:24.144923Z",
     "shell.execute_reply.started": "2023-03-08T08:07:23.062594Z",
     "shell.execute_reply": "2023-03-08T08:07:24.143947Z"
    },
    "trusted": true,
    "id": "UzFW6F2_C1OM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if SET_SVM_active:\n",
    "    modelSVM = SVC(kernel = 'linear', max_iter = 100000, verbose=True)\n",
    "    model_train(modelSVM, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression (Best Model) :"
   ],
   "metadata": {
    "id": "eR4604KRC1OM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "modelLR = LogisticRegression(max_iter=1000)\n",
    "model_train(modelLR, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:09:56.827671Z",
     "iopub.execute_input": "2023-03-08T08:09:56.828070Z",
     "iopub.status.idle": "2023-03-08T08:11:11.443944Z",
     "shell.execute_reply.started": "2023-03-08T08:09:56.828032Z",
     "shell.execute_reply": "2023-03-08T08:11:11.442942Z"
    },
    "trusted": true,
    "id": "KkkTCParC1OM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest:"
   ],
   "metadata": {
    "id": "bEGUQQvkC1OM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = RandomForestClassifier(n_estimators=100,max_depth=15,max_features='sqrt')\n",
    "model_train(model, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:20:13.896081Z",
     "iopub.execute_input": "2023-03-08T08:20:13.896506Z",
     "iopub.status.idle": "2023-03-08T08:20:37.522992Z",
     "shell.execute_reply.started": "2023-03-08T08:20:13.896471Z",
     "shell.execute_reply": "2023-03-08T08:20:37.521949Z"
    },
    "trusted": true,
    "id": "2mjAK3ZTC1ON"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"finished\")"
   ],
   "metadata": {
    "id": "A7KILiHlC1ON"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
