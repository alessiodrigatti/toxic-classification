{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data, drop empty Txt and Settings\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        comment_text  toxic\n0  u created request scholarlyarticles awaiting c...      1\n1  looking lgbt profession category point get beh...      1\n2  new york city regulation regarding removal, , ...      0\n3  thank speedy rollback would believe first time...      1\n4  want talk stuff perfectly willing refer commen...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>u created request scholarlyarticles awaiting c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>looking lgbt profession category point get beh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>new york city regulation regarding removal, , ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>thank speedy rollback would believe first time...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>want talk stuff perfectly willing refer commen...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SETTINGS\n",
    "SET_vectorizer_hash = False\n",
    "SET_vectorizer_tfidf = False\n",
    "SET_vectorizer_glove = True\n",
    "SET_vectorizer_OHE = False\n",
    "\n",
    "\n",
    "\n",
    "SET_datapath = \"data_pp_cut.json\"\n",
    "\n",
    "df = pd.read_json(SET_datapath, orient=\"columns\")\n",
    "\n",
    "df.drop(df[df.comment_text.str.len() == 0].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Embedding\n",
    "\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hash Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if SET_vectorizer_hash:\n",
    "\n",
    "    vectorizer_hash = HashingVectorizer(ngram_range=(1,2), n_features=2500)\n",
    "    X = vectorizer_hash.fit_transform(df[\"comment_text\"].values.tolist()).toarray()\n",
    "    y = df[\"toxic\"].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TFidf Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if SET_vectorizer_tfidf:\n",
    "\n",
    "    vectorizer_tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=2500)\n",
    "    X = vectorizer_tfidf.fit_transform(df[\"comment_text\"].values.tolist()).toarray()\n",
    "    y = df['toxic'].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Glove Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "def get_embedding(series, model, tokenizer):\n",
    "    embedding_matrix = []\n",
    "\n",
    "    for text in series.to_list():\n",
    "        text_vec = np.zeros(model.vector_size)\n",
    "        number_of_vectors = 0\n",
    "\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        for token in tokens:\n",
    "            number_of_vectors += 1\n",
    "\n",
    "            if token in glove_model.key_to_index:\n",
    "                text_vec = text_vec + model[token]\n",
    "\n",
    "        embedding_matrix.append( text_vec / number_of_vectors)\n",
    "\n",
    "    return np.array(embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if SET_vectorizer_glove:\n",
    "    import gensim.downloader as api\n",
    "\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "if SET_vectorizer_glove:\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "\n",
    "    X = get_embedding(df.comment_text, glove_model, tokenizer)\n",
    "    y = df.toxic.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### OneHotEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if SET_vectorizer_OHE:\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    model_ohe = encoder.fit_transform(df[[\"comment_text\"]]).toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "\"from transformers import BertTokenizer, TFBertModel\\n\\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\\n\\n# Tokenize input comments\\nencoded_comments = tokenizer(df['comment_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\\n\\n# Pass tokenized comments through BERT model to get embeddings\\nbert_embeddings = model(encoded_comments)['pooler_output'].numpy()\""
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input comments\n",
    "encoded_comments = tokenizer(df['comment_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Pass tokenized comments through BERT model to get embeddings\n",
    "bert_embeddings = model(encoded_comments)['pooler_output'].numpy()\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "\"X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, df['toxic'].values, test_size=0.2, random_state=99)\\n\\n# Scale input features using MinMaxScaler\\nscaler = MinMaxScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\""
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, df['toxic'].values, test_size=0.2, random_state=99)\n",
    "\n",
    "# Scale input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Data for Models\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T09:06:44.894984Z",
     "iopub.execute_input": "2023-03-08T09:06:44.895705Z",
     "iopub.status.idle": "2023-03-08T09:06:44.902237Z",
     "shell.execute_reply.started": "2023-03-08T09:06:44.895669Z",
     "shell.execute_reply": "2023-03-08T09:06:44.900681Z"
    },
    "trusted": true,
    "id": "BXQp4lTzC1OK"
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1181, 300)\n",
      "X_test shape: (296, 300)\n",
      "y_train shape: (1181,)\n",
      "y_test shape: (296,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Models and get Scores\n",
    "\n",
    "----"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Try... Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# load the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# convert text to numerical data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)\n",
    "X_train = tokenizer.texts_to_matrix(newsgroups_train.data, mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(newsgroups_test.data, mode='tfidf')\n",
    "\n",
    "# convert labels to categorical data\n",
    "num_classes = np.max(newsgroups_train.target) + 1\n",
    "y_train = to_categorical(newsgroups_train.target, num_classes)\n",
    "y_test = to_categorical(newsgroups_test.target, num_classes)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(2000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "#model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def model_train(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_tr = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"--------------------Training Performance---------------------\")\n",
    "    print(accuracy_score(y_train,y_pred_tr))\n",
    "    print(classification_report(y_train,y_pred_tr))\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(\"--------------------Testing Performance----------------------\")\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),cmap='viridis',annot=True,fmt='.4g')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T09:06:48.069063Z",
     "iopub.execute_input": "2023-03-08T09:06:48.069767Z",
     "iopub.status.idle": "2023-03-08T09:06:48.077451Z",
     "shell.execute_reply.started": "2023-03-08T09:06:48.069730Z",
     "shell.execute_reply": "2023-03-08T09:06:48.076100Z"
    },
    "trusted": true,
    "id": "eB0eFrl8C1OK"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Naive Bayes:"
   ],
   "metadata": {
    "id": "tfuKOJNrC1OL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "modelNB = MultinomialNB()\n",
    "model_train(modelNB, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:07:23.062257Z",
     "iopub.execute_input": "2023-03-08T08:07:23.062625Z",
     "iopub.status.idle": "2023-03-08T08:07:24.144923Z",
     "shell.execute_reply.started": "2023-03-08T08:07:23.062594Z",
     "shell.execute_reply": "2023-03-08T08:07:24.143947Z"
    },
    "trusted": true,
    "id": "UzFW6F2_C1OM"
   },
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (11314, 20) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m modelNB \u001B[38;5;241m=\u001B[39m MultinomialNB()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodelNB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m, in \u001B[0;36mmodel_train\u001B[1;34m(model, X_train, X_test, y_train, y_test)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodel_train\u001B[39m(model, X_train, X_test, y_train, y_test):\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     y_pred_tr \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_train)\n\u001B[0;32m      4\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\naive_bayes.py:749\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    729\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[0;32m    730\u001B[0m \n\u001B[0;32m    731\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;124;03m    Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m--> 749\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    750\u001B[0m _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    752\u001B[0m labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\naive_bayes.py:583\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[1;34m(self, X, y, reset)\u001B[0m\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    582\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\base.py:584\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    582\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 584\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    585\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1104\u001B[0m     )\n\u001B[0;32m   1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1107\u001B[0m     X,\n\u001B[0;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1120\u001B[0m )\n\u001B[1;32m-> 1122\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43m_check_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1124\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1143\u001B[0m, in \u001B[0;36m_check_y\u001B[1;34m(y, multi_output, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1142\u001B[0m     estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[1;32m-> 1143\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcolumn_or_1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1144\u001B[0m     _assert_all_finite(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, estimator_name\u001B[38;5;241m=\u001B[39mestimator_name)\n\u001B[0;32m   1145\u001B[0m     _ensure_no_complex_data(y)\n",
      "File \u001B[1;32m~\\Documents\\nlp_p2\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1202\u001B[0m, in \u001B[0;36mcolumn_or_1d\u001B[1;34m(y, dtype, warn)\u001B[0m\n\u001B[0;32m   1193\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1194\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA column-vector y was passed when a 1d array was\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1195\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m expected. Please change the shape of y to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1198\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   1199\u001B[0m         )\n\u001B[0;32m   1200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _asarray_with_order(xp\u001B[38;5;241m.\u001B[39mreshape(y, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m, xp\u001B[38;5;241m=\u001B[39mxp)\n\u001B[1;32m-> 1202\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1203\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my should be a 1d array, got an array of shape \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(shape)\n\u001B[0;32m   1204\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: y should be a 1d array, got an array of shape (11314, 20) instead."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression (Best Model) :"
   ],
   "metadata": {
    "id": "eR4604KRC1OM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "modelLR = LogisticRegression(max_iter=1000)\n",
    "model_train(modelLR, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:09:56.827671Z",
     "iopub.execute_input": "2023-03-08T08:09:56.828070Z",
     "iopub.status.idle": "2023-03-08T08:11:11.443944Z",
     "shell.execute_reply.started": "2023-03-08T08:09:56.828032Z",
     "shell.execute_reply": "2023-03-08T08:11:11.442942Z"
    },
    "trusted": true,
    "id": "KkkTCParC1OM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest:"
   ],
   "metadata": {
    "id": "bEGUQQvkC1OM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = RandomForestClassifier(n_estimators=100,max_depth=15,max_features='sqrt')\n",
    "model_train(model, X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-08T08:20:13.896081Z",
     "iopub.execute_input": "2023-03-08T08:20:13.896506Z",
     "iopub.status.idle": "2023-03-08T08:20:37.522992Z",
     "shell.execute_reply.started": "2023-03-08T08:20:13.896471Z",
     "shell.execute_reply": "2023-03-08T08:20:37.521949Z"
    },
    "trusted": true,
    "id": "2mjAK3ZTC1ON"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"finished\")"
   ],
   "metadata": {
    "id": "A7KILiHlC1ON"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
